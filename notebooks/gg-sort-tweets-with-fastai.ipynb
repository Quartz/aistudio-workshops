{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2FtGCN6bVsg"
   },
   "source": [
    "# Finding Fact-checkable Tweets with Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Our challenge is to find out which tweets with the hashtag `#txlege` are **checkable statements of fact**.\n",
    "\n",
    "Take a look at the [kinds of tweets in question](https://twitter.com/search?q=%23txlege&src=typed_query).\n",
    "\n",
    "Recognizing \"fact-checkability\" seems like it might be a uniquely human skill. But we can make a machine-learning mmodel that does it pretty well.\n",
    "\n",
    "### First, the language model\n",
    "\n",
    "We'll get into the details below, but here's our two-step process for this project:\n",
    "\n",
    "First, we need a model trained to recognize the patterns of English. For that, we'd need some huge dataset of English-language text. Fortunately, someone has already done that for us! We'll be using a model trained on thousands of long Wikipedia articles. It's called [wikitext-103](https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset).\n",
    "\n",
    "We'll then use _transfer learning_ (like we did in for the helicopter maps) to further train wikitext-103 on our particular corpus: several thousand #txlege tweets. So we benefit from its training on both English-language articles _and_ #txlege tweets.\n",
    "\n",
    "That will give us a **language model** that's good at detecting patterns in #txlege tweets.\n",
    "\n",
    "\n",
    "### Second, the classification model\n",
    "\n",
    "Next we need a model that will sort -- aka _classify_ -- tweets into \"fact-checkable\" and \"not fact-checkable.\" This will combine the patern-recognition embedded in the language model and examples of both kinds of tweets to make a predition on which class _new_ tweets belong to. This is our **classification model**.\n",
    "\n",
    "\n",
    "## The Plan\n",
    "\n",
    "Here's what we're going to do:\n",
    "\n",
    "- Grab files with a bunch of tweets\n",
    "- Make a **language model** from a model pretrained on Wikipedia _plus_ all the tweets as we have\n",
    "- Make a **classification model** to predict whether a given tweet is checkable or not, using tweets that were hand-labeled by folks at the Austin American-Statesman.\n",
    "- Use that classification model to predict the checkability of unseen tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "This notebook was based on one originally created by Jeremy Howard and the other folks at [fast.ai](https://fast.ai) as part of [this fantastic class](https://course.fast.ai/). Specifically, it comes from Lesson 4. You can [see the lession video](https://course.fast.ai/videos/?lesson=4) and [the original class notebook](https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson3-imdb.ipynb). \n",
    "\n",
    "The idea for the project came from Dan Keemahill at the Austin American-Statesman newspaper. Dan, Madlin Mekelburg, and others at the paper hand-coded the tweets used for the classificaiton training.\n",
    "\n",
    "For more information about this project, and details about how to use this work in the wild, check out our [Quartz AI Studio blog post about the checkable-tweets project](https://qz.ai/?p=89).\n",
    "\n",
    "-- John Keefe, [Quartz](https://qz.com), October 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For those using Google Colaboratory ...\n",
    "\n",
    "Be aware that Google Colab instances are ephemeral -- they vanish *Poof* when you close them, or after a period of sitting idle (currently 90 minutes), or if you use one for more than 12 hours.\n",
    "\n",
    "If you're using Google Colaboratory, be sure to set your runtime to \"GPU\" which speeds up your notebook for machine learning:\n",
    "\n",
    "![change runtime](https://qz-aistudio-public.s3.amazonaws.com/workshops/notebook_images/change_runtime_2.jpg)\n",
    "![pick gpu](https://qz-aistudio-public.s3.amazonaws.com/workshops/notebook_images/pick_gpu_2.jpg)\n",
    "\n",
    "Then run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ALL GOOGLE COLAB USERS RUN THIS CELL\n",
    "\n",
    "## This runs a script that installs fast.ai\n",
    "!curl -s https://course.fast.ai/setup/colab | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For those _not_ using Google Colaboratory ...\n",
    "\n",
    "This section is just for people who decide to use one of the notebooks on a system other than Google Colaboartory. \n",
    "\n",
    "Those people should run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NON-COLABORATORY USERS SHOULD RUN THIS CELL\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everybody do this ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone needs to run the next cell, which initializes the Python libraries we'll use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## AND *EVERYBODY* SHOULD RUN THIS CELL\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from fastai.text import *\n",
    "import fastai\n",
    "print(f'fastai: {fastai.__version__}')\n",
    "print(f'cuda: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be using two sets of tweets for this project:\n",
    "\n",
    "- A CSV (comma-separated values file) containing a bunch of #txlege tweets\n",
    "- A CSV of #txlege tweets that have been hand-coded as \"fact-checkable\" or \"not fact-checkable\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to download the data we'll use for this exercise\n",
    "!wget -N https://qz-aistudio-public.s3.amazonaws.com/workshops/austin_tweet_data.zip --quiet\n",
    "!unzip -q austin_tweet_data.zip\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a directory called `data` with two files of tweets. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%ls data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9TPvnzypbVsq"
   },
   "source": [
    "### Take a peek at the tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I21aOI9xbVsr"
   },
   "source": [
    "Working with Dan Keemahill and Madlin Mekelburg over a couple of weeks during the 2019 Texas state legislative session, I have have a set of 3,797 tweets humans at the Austin American-Statesman have determined are – or are not – statements that can be fact-checked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "kWwuVUg9QHo8",
    "outputId": "703590d9-71c2-4d1b-96cf-aa0a113be58a"
   },
   "outputs": [],
   "source": [
    "# Here I read the csv into a data frame I called `austin_tweets`\n",
    "# and take a look at the first few rows\n",
    "data_path = './data/'\n",
    "hand_coded_tweets = pd.read_csv(data_path + 'hand_coded_austin_tweets.csv')\n",
    "hand_coded_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the language model, we want a bunch more examples of #txlege tweets. It doesn't matter that we didn't classify them as fact-checkable or not. The language model just needs _examples_ of the kinds of tweets it might see. \n",
    "\n",
    "So I used the website [IFTTT](https://ifttt.com) and Google Spreadsheets to collect several days worth of #txlege tweets. That's what's in the  `tweet_corpus.txt` file we looked at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "18kBCGjhbVvQ",
    "outputId": "e36434d1-d47d-44c0-c6ab-82a61843ad2a"
   },
   "outputs": [],
   "source": [
    "# read in the corpus, which has one tweet per row,\n",
    "# and take a look at the first frew rows\n",
    "corpus_tweets = pd.read_csv(data_path + 'tweet_corpus.txt')\n",
    "corpus_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eym6ulq0bVuz"
   },
   "source": [
    "## Building the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62pxEwWPbVvI"
   },
   "source": [
    "First we need a model that 'understands' the rules of English, and ideally also recognizes patterns in our particular corpus, the `#txlege` tweets. This is the language model. \n",
    "\n",
    "We'll start with a language model pretrained on a thousands of Wikipedia articles called [wikitext-103](https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset). That language model has been trained to guess the next word in a sentence based on all the previous words. It has a recurrent structure with a hidden state that is updated each time it sees a new word. This hidden state thus contains information about the sentence up to that point.\n",
    "\n",
    "For our project, we want to infuse the Wikitext model with our particular dataset – the #txlege tweets. Because the English of #txlege tweets isn't the same as the English of Wikipedia, we'll adjust the internal parameters of the model by a little bit. That includes adding words that might be extremely common in the tweets but would be barely present in wikipedia–and therefore might not be part of the vocabulary the model was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FT0C6-vpbVvK"
   },
   "source": [
    "### Using all of our tweets for the language model\n",
    "\n",
    "We want as many tweets for the language model as possible to learn the patterns of #txlege tweets.\n",
    "\n",
    "We'll start with the text of the 3,797 \"hand-coded\" tweets (though for the language model, we ignore the checkable/not checkable part of that file).\n",
    "\n",
    "To that, we'll add the additional tweets I collected in `tweet_corpus.txt`. We'll mush those all into one big file of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "aQyQIh_vNhrH",
    "outputId": "9a6afe30-01c4-4445-d7a7-8c835d7d62fc"
   },
   "outputs": [],
   "source": [
    "# here I concatenate the two tweet sets into one big set\n",
    "lm_tweets = pd.concat([hand_coded_tweets,corpus_tweets], sort=True)\n",
    "\n",
    "# as a sanity check, let's look at the size of each set, \n",
    "# and then the ontatenated set\n",
    "print('hand coded tweets:', len(hand_coded_tweets) )\n",
    "print('corpus of tweets:', len(corpus_tweets) )\n",
    "print('total tweets:', len(lm_tweets) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g55U1v3mbVvm"
   },
   "source": [
    "Great: Now we have 7,485 tweets to use for the language model.\n",
    "\n",
    "One thing to note ... the first set had two columns, `checkable` and `tweet_text`, while the corpus had just one collumn,  `tweet_text`. The combined has the original two columns, though many of the entries will be `NaN` for \"not a number.\" Thats okay, because we're only going to use the `tweet_text` column for the language model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "pFp92p9NbVvs"
   },
   "outputs": [],
   "source": [
    "# Saving as csv for easier reading in a moment\n",
    "lm_tweets.to_csv(data_path + 'lm_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3fU_R9lxbVv7"
   },
   "source": [
    "Fast.ai uses a concept called a \"[data bunch](https://docs.fast.ai/basic_data.html)\" to handle machine-learning data, which takes care of a lot of the more fickle machine-learning data preparation.\n",
    "\n",
    "We have to use a special kind of data bunch for the language model, one that ignores the labels, and will shuffle the texts at each epoch before concatenating them all together (only the training set gets shuffled; we don't shuffle for the validation set). It will also create batches that read the text in order with targets (aka the best guesses) that are the next word in the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "C2rJuWVCbVv5"
   },
   "outputs": [],
   "source": [
    "# Loading in data with the TextLMDataBunch factory class, using all the defaults\n",
    "data_lm = TextLMDataBunch.from_csv(data_path, 'lm_tweets.csv', text_cols='tweet_text', label_cols='checkable')\n",
    "data_lm.save('data_lm_tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rY0LkJCQbVwF"
   },
   "source": [
    "We can then put all of our tweets (now stored in `data_lm`) into a learner object along with the pretrained Wikitext model -- here called `AWD_LTSM`, which is downloaded the first time you'll execute the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EiiXrA6zbVwH"
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfAXUyLnOB_J"
   },
   "source": [
    "One of the most important settings when we actually _train_ our model is the **learning rate**. I'm not going to dive into it here (though I encourage you to explore it), but will use a fast.ai tool to find the best learning rate to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "W7b-Z94dbVwL",
    "outputId": "f6044993-11ab-4889-c0fe-892243e61a2e"
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Un204GpfbVwO",
    "outputId": "607fdc89-064b-427b-e92e-f35bdc868923"
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "td3D3mK8TtHa"
   },
   "source": [
    "This gives us a graph of the optimal learning rate ... which is the point where the graph really dives downward (`1e-02`). Again, there's much more on picking and learning rates in the fast.ai course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQA3dZWPachE"
   },
   "source": [
    "Now we can train the Language Model. (Essentailly, we're training it to be good at guessing the *next word* in a sentence, given all of the previous words.)\n",
    "\n",
    "The variabales we're passing are `1` to just do one cycle of learning, the learning rate of `1e-2`, and some momentum settings we won't get into here -- but these are pretty safe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "pveLnA6kbVwQ",
    "outputId": "38ef873c-5c2a-461d-9688-d8f4e1edbd5f"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "o-dYIVFcbVwS",
    "outputId": "aa2db805-6429-4592-f905-0479b9872820"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-1, moms=(0.8,0.7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M8kZOKSybVwX"
   },
   "source": [
    "To complete the fine-tuning, we \"unfreeze\" the original wikitext-103 language model and let our new training efforts work their way into the original neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "kTfuNCuhbVwX",
    "outputId": "1e99e6bc-8421-4d63-faf3-4c16fb3f07f6"
   },
   "outputs": [],
   "source": [
    "# This takes a couple of minutes!\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, 1e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fv0mZuVhbVwd"
   },
   "source": [
    "While our accuracy may _seem_ low ... in this case it means the language model correctly guessed the next word in a sentence more than 1/3 of the time. That's pretty good! And we can see that even when it's wrong, it makes some pretty \"logical\" guesses. \n",
    "\n",
    "Let's give it a starting phrase and see how it does:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_Wws03kmbVwd",
    "outputId": "dd545482-c5c8-4e93-a79e-64578e6478a3"
   },
   "outputs": [],
   "source": [
    "TEXT = \"I wonder if this\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 3\n",
    "\n",
    "print(\"\\n\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hj3aD0UebVwr"
   },
   "source": [
    "Remember, these are not real ... they were _generated_ by the model when it tried to guess each of the next words in the sentence! Generating text like this is not why we made the language model (though you can see where text-generation AI starts from!)\n",
    "\n",
    "Also note that the model is often crafting the response _in the form of a tweet!_\n",
    "\n",
    "We now save the model's encoder, which is the mathematical representation of what the language model \"understands\" about English patterns infused by our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "thkQAeQ2bVwr"
   },
   "outputs": [],
   "source": [
    "learn.save_encoder('fine_tuned_enc')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zt6pDPEDbVww"
   },
   "source": [
    "## Building the classifier model\n",
    "\n",
    "This is the model that will use our langauge model **and** the hand-coded tweets to guess if new tweets are fact-checkable or not.\n",
    "\n",
    "We'll create a new data bunch that only grabs the hand-coded tweets and keeps track of the labels there (true or false, for fact-checkability). We also pass in the `vocab` -- which is the list of the most useful words from the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ntIAhZbbbVw1"
   },
   "outputs": [],
   "source": [
    "data_clas = TextClasDataBunch.from_csv(data_path, 'hand_coded_austin_tweets.csv', vocab=data_lm.vocab, text_cols='tweet_text', label_cols='checkable')\n",
    "\n",
    "data_clas.save('data_clas_tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WCwf-DyEbVxA"
   },
   "source": [
    "We can then create a model to classify tweets. You can see that in the next two lines we include the processed, hand-coded tweets (`data_clas`), the original Wikitext model (`AWD_LSTM`), and the knowledge we saved after infusing the language model with tweets (`fine_tuned_enc`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xlJKU0g3bVxA"
   },
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n",
    "learn.load_encoder('fine_tuned_enc');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Az5OuN6_hJaF"
   },
   "source": [
    "With neural networks, there are lots of tweaks you can adjust — known as \"hyperparameters\" — such as learning rate and momentum. The fast.ai defaults are pretty great, and the tools it has for finding the learning rate are super useful. I'm going to skip those details here for now. There's more to learn at [qz.ai](https://qz.ai) or at the [this great fast.ai course](https://course.fast.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "S5g2xmtlbVxJ",
    "outputId": "79971cc2-dde2-4dfc-a340-7dcae79f2bdd"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, 1e-2, moms=(0.8,0.7))\n",
    "\n",
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(2, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))\n",
    "\n",
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(2, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J-TSQ7iDTdIG"
   },
   "source": [
    "Let's give it an example ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "mBAXEa2MbVxa",
    "outputId": "4163f4ee-f9e0-4d93-b5a2-c47e38a78205"
   },
   "outputs": [],
   "source": [
    "example = \"Four states have two universities represented in the top 20 highest-paid executives of public colleges. Texas has SIX\"\n",
    "learn.predict(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8u1ptJu1UmT9"
   },
   "source": [
    "`True` means checkable! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can open the \"black box\" a little to see what words the model is keying into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interp = TextClassificationInterpretation.from_learner(learn) \n",
    "interp.show_intrinsic_attention(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSOgwF1ybVyh"
   },
   "source": [
    "## Saving to Google Drive\n",
    "\n",
    "At present, your Google Colaboratory Notebook disappears when you close it — along with all of your data. If you'd like to save your model to your Google Drive, run the following cell and grant the permissions it requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "root_dir = \"/content/gdrive/My Drive/\"\n",
    "base_dir = root_dir + 'ai-workshop/checkable_tweet_models/'\n",
    "save_path = Path(base_dir)\n",
    "save_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line will save everything we need for predictions to a file to your Google Drive in the `ai-workshops` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.export(save_path/\"export.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, to load the model into your code, connect to your Google drive using the same block above that starts `from google.colab import drive ...` and then run this:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model from the 'export.pkl' file on your Google Drive\n",
    "learn = load_learner(save_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "57uA-44kVh_T"
   },
   "source": [
    "We used a model built this way to classify #txlege tweets as they were tweeted. For details about deploying a predictor in the cloud using Render, see our [blog post about building the checkable-tweets project](https://qz.ai/?p=89)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "quartz-fastai-checkable-tweets.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
